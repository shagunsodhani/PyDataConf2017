{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Network measures similarity between two comparable items "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use cases\n",
    "    \n",
    "### [One-Shot Learning](https://hackernoon.com/one-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e)\n",
    "### Are two photographs of the same person\n",
    "### Are two questions paraphrases of each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/shagunsodhani/PyDataConf2017/master/assets/siamese.jpeg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://raw.githubusercontent.com/shagunsodhani/PyDataConf2017/master/assets/siamese.jpeg\")\n",
    "# Image taken from: https://hackernoon.com/one-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Important Features\n",
    "\n",
    "### Similarity Vs Classification\n",
    "### Weight sharing\n",
    "### Feature representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_dataset = \"/home/shagun/FortKnox/Quora/quora_duplicate_questions.tsv\"\n",
    "path_to_glove_vectors = \"/home/shagun/models/GloVe/glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "embedding_dim = 100\n",
    "# Refer the exploratory notebook to see how max_len value is arrived at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of question pairs =  404290\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv(path_to_dataset, delimiter=\"\\t\")\n",
    "print(\"Total number of question pairs = \", str(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>368298</th>\n",
       "      <td>368298</td>\n",
       "      <td>498659</td>\n",
       "      <td>498660</td>\n",
       "      <td>I have been exploring the field of Social Medi...</td>\n",
       "      <td>How ban of 500rs &amp; 100 rs notes lead to rise o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193390</th>\n",
       "      <td>193390</td>\n",
       "      <td>293283</td>\n",
       "      <td>293284</td>\n",
       "      <td>How do you determine the speed of an electroma...</td>\n",
       "      <td>What is the speed of electromagnetic waves? Ho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51735</th>\n",
       "      <td>51735</td>\n",
       "      <td>91727</td>\n",
       "      <td>91728</td>\n",
       "      <td>Is Apple going to discontinue the MacBook Air?</td>\n",
       "      <td>Is Apple going to stop making the MacBook air?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256858</th>\n",
       "      <td>256858</td>\n",
       "      <td>110818</td>\n",
       "      <td>372075</td>\n",
       "      <td>When should you downvote a comment?</td>\n",
       "      <td>Should you comment when you downvote answers?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185934</th>\n",
       "      <td>185934</td>\n",
       "      <td>96187</td>\n",
       "      <td>259382</td>\n",
       "      <td>What is the best way to lose 40 pounds in 3 we...</td>\n",
       "      <td>What are some successful, healthy ways to lose...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "368298  368298  498659  498660   \n",
       "193390  193390  293283  293284   \n",
       "51735    51735   91727   91728   \n",
       "256858  256858  110818  372075   \n",
       "185934  185934   96187  259382   \n",
       "\n",
       "                                                question1  \\\n",
       "368298  I have been exploring the field of Social Medi...   \n",
       "193390  How do you determine the speed of an electroma...   \n",
       "51735      Is Apple going to discontinue the MacBook Air?   \n",
       "256858                When should you downvote a comment?   \n",
       "185934  What is the best way to lose 40 pounds in 3 we...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "368298  How ban of 500rs & 100 rs notes lead to rise o...             0  \n",
       "193390  What is the speed of electromagnetic waves? Ho...             1  \n",
       "51735      Is Apple going to stop making the MacBook air?             1  \n",
       "256858      Should you comment when you downvote answers?             0  \n",
       "185934  What are some successful, healthy ways to lose...             0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us look at a sample of the dataset\n",
    "df_sample = df.sample(5)\n",
    "\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will play with a very small sample of this dataset to save on time. Feel free to train the network on the entire data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = list(df['is_duplicate'].apply(lambda x: int(x)).values)\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of all the question pairs\n",
    "first_question_list = list(df['question1'].apply(lambda x: str(x)).values)\n",
    "second_question_list = list(df['question2'].apply(lambda x: str(x)).values)\n",
    "question_list = list(zip(first_question_list, second_question_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(question_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/shagunsodhani/PyDataConf2017/master/assets/keras-tensorflow-logo.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://raw.githubusercontent.com/shagunsodhani/PyDataConf2017/master/assets/keras-tensorflow-logo.jpg\")\n",
    "# Image taken from: https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from utils.util import *\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers import Embedding, Input, GRU, Dense, Activation, Lambda, BatchNormalization\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(first_question_list + second_question_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence1 = pad_sequences(tokenizer.texts_to_sequences(first_question_list), maxlen=max_len)\n",
    "sequence2 = pad_sequences(tokenizer.texts_to_sequences(second_question_list), maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go\n"
     ]
    }
   ],
   "source": [
    "if(len(sequence1) == len(sequence2)):\n",
    "    print(\"Good to go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basic ML Preprocessing\n",
    "indices = np.arange(len(sequence1))\n",
    "np.random.shuffle(indices)\n",
    "sequence1 = sequence1[indices]\n",
    "sequence2 = sequence2[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(0.3 * len(sequence1))\n",
    "\n",
    "sequence1_train = sequence1[:-nb_validation_samples]\n",
    "sequence2_train = sequence2[:-nb_validation_samples]\n",
    "labels_train = labels[:-nb_validation_samples]\n",
    "sequence1_val = sequence1[-nb_validation_samples:]\n",
    "sequence2_val = sequence2[-nb_validation_samples:]\n",
    "labels_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 700\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples: \" + str(len(sequence1_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation examples: 300\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of validation examples: \" + str(len(sequence1_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "embeddings_index = get_glove_embeddings(path_to_glove_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the embedding matrix which our model would use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparing the embedding matrix which our model would use\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the question into a vector space using a Bidirectional GRU (or LSTM or whatever RNN you believe in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def question_encoder():\n",
    "    return Bidirectional(GRU(units=200), merge_mode='concat', name=\"bidir_gru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next part is the core of this network and we would walk through it slowly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_question_network():\n",
    "    \n",
    "#     Create an input layer\n",
    "    sequence_input = Input(shape=(max_len,), dtype='int32', name=\"input_layer\")\n",
    "    \n",
    "#     Create an embedding layer\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_len,\n",
    "                            trainable=False)\n",
    "    \n",
    "#     Use the embedding layer we just created\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "#     Embeddings are for words, sentences uses encoders\n",
    "    encoded_question = question_encoder()(embedded_sequences)\n",
    "    \n",
    "#     Lets fully connect them\n",
    "    dense1 = Dense(128)(encoded_question)\n",
    "    relu1 = Activation('relu')(dense1)\n",
    "    \n",
    "#    Make it deep\n",
    "    dense2 = Dense(64)(relu1)\n",
    "    relu2 = Activation('relu')(dense2)    \n",
    "\n",
    "#     And Deeper\n",
    "    dense3 = Dense(32)(relu2)\n",
    "    relu3 = Activation('relu')(dense3)\n",
    "\n",
    "#     Now we are in rythm\n",
    "    dense4 = Dense(16)(relu3)\n",
    "    tanh4 = Activation('relu')(dense4)\n",
    "    \n",
    "    output = BatchNormalization()(tanh4)\n",
    "    \n",
    "    model = Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will make the siamese twin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_network():\n",
    "    # network definition\n",
    "    question_network = create_question_network()\n",
    "    \n",
    "#     input to the first head of the network\n",
    "    input1 = Input(shape=(max_len,))\n",
    "    \n",
    "#     input to the second head of the network\n",
    "    input2 = Input(shape=(max_len,))\n",
    "    \n",
    "#     processing the first input\n",
    "    processed1 = question_network(input1)\n",
    "    \n",
    "#     processing the second input\n",
    "    processed2 = question_network(input2)\n",
    "    \n",
    "#     Computing the distance between the transformed inputs.\n",
    "    distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed1, processed2])\n",
    "\n",
    "    \n",
    "    model = Model(inputs=[input1, input2], outputs=distance)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 13s - loss: 1.2265 - val_loss: 2.3629\n",
      "Training accuracy: 0.377142857143\n",
      "Validation accuracy: 0.346666666667\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 13s - loss: 0.9308 - val_loss: 2.3534\n",
      "Training accuracy: 0.377142857143\n",
      "Validation accuracy: 0.346666666667\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 17s - loss: 0.7037 - val_loss: 2.2903\n",
      "Training accuracy: 0.378571428571\n",
      "Validation accuracy: 0.346666666667\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 10s - loss: 0.6211 - val_loss: 2.3169\n",
      "Training accuracy: 0.378571428571\n",
      "Validation accuracy: 0.346666666667\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 10s - loss: 0.5178 - val_loss: 2.2878\n",
      "Training accuracy: 0.378571428571\n",
      "Validation accuracy: 0.346666666667\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 11s - loss: 0.4087 - val_loss: 2.2185\n",
      "Training accuracy: 0.378571428571\n",
      "Validation accuracy: 0.353333333333\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 11s - loss: 0.3417 - val_loss: 2.2216\n",
      "Training accuracy: 0.38\n",
      "Validation accuracy: 0.353333333333\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 11s - loss: 0.2901 - val_loss: 2.1616\n",
      "Training accuracy: 0.381428571429\n",
      "Validation accuracy: 0.36\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 11s - loss: 0.2604 - val_loss: 2.1356\n",
      "Training accuracy: 0.384285714286\n",
      "Validation accuracy: 0.356666666667\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 10s - loss: 0.2341 - val_loss: 2.1022\n",
      "Training accuracy: 0.39\n",
      "Validation accuracy: 0.356666666667\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 10s - loss: 0.1920 - val_loss: 2.0545\n",
      "Training accuracy: 0.395714285714\n",
      "Validation accuracy: 0.376666666667\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 10s - loss: 0.1697 - val_loss: 2.0282\n",
      "Training accuracy: 0.395714285714\n",
      "Validation accuracy: 0.39\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 10s - loss: 0.1510 - val_loss: 1.9906\n",
      "Training accuracy: 0.404285714286\n",
      "Validation accuracy: 0.4\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 10s - loss: 0.1380 - val_loss: 1.9583\n",
      "Training accuracy: 0.437142857143\n",
      "Validation accuracy: 0.41\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 10s - loss: 0.1230 - val_loss: 1.9258\n",
      "Training accuracy: 0.452857142857\n",
      "Validation accuracy: 0.433333333333\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 15s - loss: 0.1250 - val_loss: 1.8394\n",
      "Training accuracy: 0.54\n",
      "Validation accuracy: 0.473333333333\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 12s - loss: 0.1102 - val_loss: 1.8318\n",
      "Training accuracy: 0.532857142857\n",
      "Validation accuracy: 0.476666666667\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 15s - loss: 0.1043 - val_loss: 1.7987\n",
      "Training accuracy: 0.595714285714\n",
      "Validation accuracy: 0.493333333333\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 12s - loss: 0.1067 - val_loss: 1.7173\n",
      "Training accuracy: 0.707142857143\n",
      "Validation accuracy: 0.506666666667\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 15s - loss: 0.1137 - val_loss: 1.6787\n",
      "Training accuracy: 0.805714285714\n",
      "Validation accuracy: 0.546666666667\n"
     ]
    }
   ],
   "source": [
    "model = create_network()\n",
    "\n",
    "optimizer = Adam(lr=0.001, clipnorm=5)\n",
    "model.compile(loss=contrastive_loss, optimizer=optimizer)\n",
    "\n",
    "for i in range(20):\n",
    "    model.fit([sequence1_train, sequence2_train], labels_train,\n",
    "       validation_data=([sequence1_val, sequence2_val], labels_val),\n",
    "       batch_size=128, epochs=1)\n",
    "    \n",
    "    model_labels_train = model.predict([sequence1_train, sequence2_train], batch_size=128)\n",
    "    print(\"Training accuracy: \"+str(compute_accuracy(model_labels_train, labels_train)))\n",
    "    \n",
    "    model_labels_val = model.predict([sequence1_val, sequence2_val], batch_size=128)\n",
    "    print(\"Validation accuracy: \"+str(compute_accuracy(model_labels_val, labels_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_labels_val.ravel() < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(np.equal(model_labels_val.ravel() < 0.5, labels_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compute_accuracy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
