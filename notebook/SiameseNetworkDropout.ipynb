{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refer the notebook on Siamese Networks, Batch Normalisation and Skip Connections before using this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why [Dropout](http://jmlr.org/papers/v15/srivastava14a.html)\n",
    "\n",
    "\n",
    "## Regularize Training.\n",
    "\n",
    "## Helps in generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://github.com/shagunsodhani/PyDataConf2017/blob/master/assets/dropout.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_dataset = \"/home/shagun/FortKnox/Quora/quora_duplicate_questions.tsv\"\n",
    "path_to_glove_vectors = \"/home/shagun/models/GloVe/glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "embedding_dim = 100\n",
    "# Refer the exploratory notebook to see how max_len value is arrived at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of question pairs =  404290\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv(path_to_dataset, delimiter=\"\\t\")\n",
    "print(\"Total number of question pairs = \", str(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>289783</th>\n",
       "      <td>289783</td>\n",
       "      <td>410957</td>\n",
       "      <td>410958</td>\n",
       "      <td>What are some important international news tha...</td>\n",
       "      <td>What are some instances of your adult cat acti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175085</th>\n",
       "      <td>175085</td>\n",
       "      <td>107022</td>\n",
       "      <td>122068</td>\n",
       "      <td>What is GST bill and what are its implications?</td>\n",
       "      <td>What is this GST bill all about?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16059</th>\n",
       "      <td>16059</td>\n",
       "      <td>30638</td>\n",
       "      <td>30639</td>\n",
       "      <td>Does it hurt horses to walk on paved roads?</td>\n",
       "      <td>Does it hurt when horses walk on paved roads?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180763</th>\n",
       "      <td>180763</td>\n",
       "      <td>242308</td>\n",
       "      <td>11353</td>\n",
       "      <td>How Can I lose more weight from my face?</td>\n",
       "      <td>How should I lose weight from my face?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113420</th>\n",
       "      <td>113420</td>\n",
       "      <td>53324</td>\n",
       "      <td>12719</td>\n",
       "      <td>What are the safety precautions on handling sh...</td>\n",
       "      <td>What are the safety precautions on handling sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "289783  289783  410957  410958   \n",
       "175085  175085  107022  122068   \n",
       "16059    16059   30638   30639   \n",
       "180763  180763  242308   11353   \n",
       "113420  113420   53324   12719   \n",
       "\n",
       "                                                question1  \\\n",
       "289783  What are some important international news tha...   \n",
       "175085    What is GST bill and what are its implications?   \n",
       "16059         Does it hurt horses to walk on paved roads?   \n",
       "180763           How Can I lose more weight from my face?   \n",
       "113420  What are the safety precautions on handling sh...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "289783  What are some instances of your adult cat acti...             0  \n",
       "175085                   What is this GST bill all about?             0  \n",
       "16059       Does it hurt when horses walk on paved roads?             1  \n",
       "180763             How should I lose weight from my face?             1  \n",
       "113420  What are the safety precautions on handling sh...             1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us look at a sample of the dataset\n",
    "df_sample = df.sample(5)\n",
    "\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will play with a very small sample of this dataset to save on time. Feel free to train the network on the entire data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = list(df['is_duplicate'].apply(lambda x: int(x)).values)\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of all the question pairs\n",
    "first_question_list = list(df['question1'].apply(lambda x: str(x)).values)\n",
    "second_question_list = list(df['question2'].apply(lambda x: str(x)).values)\n",
    "question_list = list(zip(first_question_list, second_question_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(question_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/shagunsodhani/PyDataConf2017/master/assets/keras-tensorflow-logo.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://raw.githubusercontent.com/shagunsodhani/PyDataConf2017/master/assets/keras-tensorflow-logo.jpg\")\n",
    "# Image taken from: https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from utils.util import *\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers import Embedding, Input, GRU, Dense, Activation, Lambda, BatchNormalization, Dropout\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(first_question_list + second_question_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence1 = pad_sequences(tokenizer.texts_to_sequences(first_question_list), maxlen=max_len)\n",
    "sequence2 = pad_sequences(tokenizer.texts_to_sequences(second_question_list), maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go\n"
     ]
    }
   ],
   "source": [
    "if(len(sequence1) == len(sequence2)):\n",
    "    print(\"Good to go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basic ML Preprocessing\n",
    "indices = np.arange(len(sequence1))\n",
    "np.random.shuffle(indices)\n",
    "sequence1 = sequence1[indices]\n",
    "sequence2 = sequence2[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(0.3 * len(sequence1))\n",
    "\n",
    "sequence1_train = sequence1[:-nb_validation_samples]\n",
    "sequence2_train = sequence2[:-nb_validation_samples]\n",
    "labels_train = labels[:-nb_validation_samples]\n",
    "sequence1_val = sequence1[-nb_validation_samples:]\n",
    "sequence2_val = sequence2[-nb_validation_samples:]\n",
    "labels_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 7000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples: \" + str(len(sequence1_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation examples: 3000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of validation examples: \" + str(len(sequence1_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "embeddings_index = get_glove_embeddings(path_to_glove_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the embedding matrix which our model would use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparing the embedding matrix which our model would use\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the question into a vector space using a Bidirectional GRU (or LSTM or whatever RNN you believe in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def question_encoder():\n",
    "    return Bidirectional(GRU(units=200), merge_mode='concat', name=\"bidir_gru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next part is the core of this network and we would walk through it slowly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_question_network():\n",
    "    \n",
    "#     Create an input layer\n",
    "    sequence_input = Input(shape=(max_len,), dtype='int32', name=\"input_layer\")\n",
    "    \n",
    "#     Create an embedding layer\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_len,\n",
    "                            mask_zero = True,\n",
    "                            trainable=False)\n",
    "    \n",
    "#     Use the embedding layer we just created\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "#     Embeddings are for words, sentences uses encoders\n",
    "    encoded_question = question_encoder()(embedded_sequences)\n",
    "    \n",
    "    dropout_prob = 0.4\n",
    "#     Lets fully connect them\n",
    "    dense1 = Dense(64)(encoded_question)\n",
    "#     Dropout\n",
    "    dense1 = Dropout(dropout_prob)(dense1)\n",
    "#     BatchNormalization\n",
    "    bn1 = BatchNormalization()(dense1)\n",
    "    relu1 = Activation('relu')(bn1)\n",
    "    \n",
    "#    Make it deep\n",
    "    dense2 = Dense(64)(relu1)\n",
    "#     Dropout\n",
    "    dense2 = Dropout(dropout_prob)(dense2)\n",
    "#     BatchNormalization\n",
    "    bn2 = BatchNormalization()(dense2)\n",
    "#     Skip Connection\n",
    "    res2 = add([relu1, bn2])\n",
    "    relu2 = Activation('relu')(res2)    \n",
    "\n",
    "#     And Deeper\n",
    "    dense3 = Dense(64)(relu2)\n",
    "#     Dropout\n",
    "    dense3 = Dropout(dropout_prob)(dense3)\n",
    "#     BatchNormalization\n",
    "    bn3 = BatchNormalization()(dense3)\n",
    "#     Skip Connection\n",
    "    res3 = add([relu2, bn3])\n",
    "    relu3 = Activation('relu')(res3)\n",
    "\n",
    "#     Now we are in rythm\n",
    "    dense4 = Dense(64)(relu3)\n",
    "#     Dropout\n",
    "    dense4 = Dropout(dropout_prob)(dense4)\n",
    "#     BatchNormalization\n",
    "    bn4 = BatchNormalization()(dense4)\n",
    "#     Skip Connection\n",
    "    res4 = add([relu3, bn4])\n",
    "    relu4 = Activation('relu')(bn4)\n",
    "\n",
    "    features = concatenate([relu4, relu3, relu2, relu1])\n",
    "#     BatchNormalization\n",
    "    output = BatchNormalization()(features)\n",
    "    \n",
    "    model = Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will make the siamese twin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_network():\n",
    "    # network definition\n",
    "    question_network = create_question_network()\n",
    "    \n",
    "#     input to the first head of the network\n",
    "    input1 = Input(shape=(max_len,))\n",
    "    \n",
    "#     input to the second head of the network\n",
    "    input2 = Input(shape=(max_len,))\n",
    "    \n",
    "#     processing the first input\n",
    "    processed1 = question_network(input1)\n",
    "    \n",
    "#     processing the second input\n",
    "    processed2 = question_network(input2)\n",
    "    \n",
    "#     Computing the distance between the transformed inputs.\n",
    "    distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed1, processed2])\n",
    "\n",
    "    \n",
    "    model = Model(inputs=[input1, input2], outputs=distance)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      "7000/7000 [==============================] - 124s - loss: 155.8288 - val_loss: 11.4491\n",
      "Training accuracy: 0.622571428571\n",
      "Validation accuracy: 0.627666666667\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      "7000/7000 [==============================] - 161s - loss: 113.7380 - val_loss: 184.5496\n",
      "Training accuracy: 0.624714285714\n",
      "Validation accuracy: 0.628666666667\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      "7000/7000 [==============================] - 132s - loss: 85.5662 - val_loss: 17.7574\n",
      "Training accuracy: 0.621714285714\n",
      "Validation accuracy: 0.623333333333\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      "7000/7000 [==============================] - 125s - loss: 67.9190 - val_loss: 23.4337\n",
      "Training accuracy: 0.620428571429\n",
      "Validation accuracy: 0.624\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      "7000/7000 [==============================] - 153s - loss: 54.8989 - val_loss: 19.6960\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-3641374d7a0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m        batch_size=128, epochs=1)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel_labels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msequence1_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence2_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training accuracy: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_labels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1515\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1517\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = create_network()\n",
    "\n",
    "optimizer = Adam(lr=0.001, clipnorm=5)\n",
    "model.compile(loss=contrastive_loss, optimizer=optimizer)\n",
    "\n",
    "for i in range(10):\n",
    "    model.fit([sequence1_train, sequence2_train], labels_train,\n",
    "       validation_data=([sequence1_val, sequence2_val], labels_val),\n",
    "       batch_size=128, epochs=1)\n",
    "    \n",
    "    model_labels_train = model.predict([sequence1_train, sequence2_train], batch_size=128)\n",
    "    print(\"Training accuracy: \"+str(compute_accuracy(model_labels_train, labels_train)))\n",
    "    \n",
    "    model_labels_val = model.predict([sequence1_val, sequence2_val], batch_size=128)\n",
    "    print(\"Validation accuracy: \"+str(compute_accuracy(model_labels_val, labels_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_labels_val.ravel() < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(np.equal(model_labels_val.ravel() < 0.5, labels_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compute_accuracy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
